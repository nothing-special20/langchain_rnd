{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85cc84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import datetime\n",
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "import transformers\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.schema.runnable.passthrough import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline, HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "#read the environment variables from the .env file\n",
    "from dotenv import dotenv_values\n",
    "# print(load_dotenv())\n",
    "config = dotenv_values(\".env\")\n",
    "HUGGINGFACEHUB_API_TOKEN = config['HUGGINGFACEHUB_API_TOKEN']\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "device_type = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387846ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_type,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f68c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# # Activate 4-bit precision base model loading\n",
    "# use_4bit = True\n",
    "\n",
    "# # Compute dtype for 4-bit base models\n",
    "# bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# # Quantization type (fp4 or nf4)\n",
    "# bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# # Activate nested quantization for 4-bit base models (double quantization)\n",
    "# use_nested_quant = False\n",
    "\n",
    "# #################################################################\n",
    "# # Set up quantization config\n",
    "# #################################################################\n",
    "# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=use_4bit,\n",
    "#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=use_nested_quant,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7552c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "\n",
    "# def print_number_of_trainable_model_parameters(model):\n",
    "#     trainable_model_params = 0\n",
    "#     all_model_params = 0\n",
    "#     for _, param in model.named_parameters():\n",
    "#         all_model_params += param.numel()\n",
    "#         if param.requires_grad:\n",
    "#             trainable_model_params += param.numel()\n",
    "#     return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "# print(print_number_of_trainable_model_parameters(model))\n",
    "\n",
    "# text_generation_pipeline = transformers.pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     task=\"text-generation\",\n",
    "#     temperature=0.2,\n",
    "#     repetition_penalty=1.1,\n",
    "#     return_full_text=True,\n",
    "#     max_new_tokens=1000,\n",
    "# )\n",
    "\n",
    "# mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline, device_map=\"mps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'question', 'context'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/robquin/Documents/Professional/Entrepreneur/Bill More Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Create llm chain\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mmistral_llm_endpoint, prompt\u001b[39m=\u001b[39mprompt)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m llm_chain\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:288\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     include_run_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    263\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_inputs(inputs)\n\u001b[1;32m    289\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[1;32m    290\u001b[0m         callbacks,\n\u001b[1;32m    291\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:445\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    443\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    444\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:197\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'question', 'context'}"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "fantasy football knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "endpoint_url = \"https://api-inference.huggingface.co/models/\" + model_name\n",
    "#currently only ('text2text-generation', 'text-generation', 'summarization') are supporte\n",
    "mistral_llm_endpoint = HuggingFaceEndpoint(endpoint_url=endpoint_url, task=\"text-generation\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
    "\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain\n",
    "llm_chain = LLMChain(llm=mistral_llm_endpoint, prompt=prompt)\n",
    "llm_chain.__call__(inputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e627d457",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/robquin/Documents/Professional/Entrepreneur/Bill More Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Scrapes the blogs above\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loader \u001b[39m=\u001b[39m AsyncChromiumLoader(articles)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robquin/Documents/Professional/Entrepreneur/Bill%20More%20Tech/misc_ai_folders/python-weaviate/Mistral_RAG.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/langchain/document_loaders/chromium.py:90\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    Load and return all Documents from the provided URLs.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy_load())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/langchain/document_loaders/chromium.py:77\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mLazily load text content from the provided URLs.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murls:\n\u001b[0;32m---> 77\u001b[0m     html_content \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascrape_playwright(url))\n\u001b[1;32m     78\u001b[0m     metadata \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: url}\n\u001b[1;32m     79\u001b[0m     \u001b[39myield\u001b[39;00m Document(page_content\u001b[39m=\u001b[39mhtml_content, metadata\u001b[39m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     36\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_once()\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:107\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     heappop(scheduled)\n\u001b[1;32m    102\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    104\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    105\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mselect(timeout)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    110\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/selectors.py:561\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    560\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     kev_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mcontrol(\u001b[39mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    562\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\n",
    "    \"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
    "    \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
    "    \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
    "    \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
    "    \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\",\n",
    "]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 558, which is longer than the specified 200\n",
      "Created a chunk of size 831, which is longer than the specified 200\n",
      "Created a chunk of size 830, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "Created a chunk of size 1391, which is longer than the specified 200\n",
      "Created a chunk of size 674, which is longer than the specified 200\n",
      "Created a chunk of size 627, which is longer than the specified 200\n",
      "Created a chunk of size 1165, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 286, which is longer than the specified 200\n",
      "Created a chunk of size 421, which is longer than the specified 200\n",
      "Created a chunk of size 470, which is longer than the specified 200\n",
      "Created a chunk of size 497, which is longer than the specified 200\n",
      "Created a chunk of size 577, which is longer than the specified 200\n",
      "Created a chunk of size 241, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 615, which is longer than the specified 200\n",
      "Created a chunk of size 403, which is longer than the specified 200\n",
      "Created a chunk of size 455, which is longer than the specified 200\n",
      "Created a chunk of size 371, which is longer than the specified 200\n",
      "Created a chunk of size 791, which is longer than the specified 200\n",
      "Created a chunk of size 501, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 1355, which is longer than the specified 200\n",
      "Created a chunk of size 674, which is longer than the specified 200\n",
      "Created a chunk of size 627, which is longer than the specified 200\n",
      "Created a chunk of size 1165, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 1387, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "# Create a deep copy of docs\n",
    "docs_copy = copy.deepcopy(docs)\n",
    "\n",
    "# Converts HTML to plain text\n",
    "html2text = Html2TextTransformer()\n",
    "\n",
    "_docs_transformed = html2text.transform_documents(docs_copy)\n",
    "\n",
    "docs_transformed = []\n",
    "\n",
    "for doc in _docs_transformed:\n",
    "    try:\n",
    "        metadata = copy.deepcopy(doc.metadata)\n",
    "        page_content = copy.deepcopy(doc.page_content)\n",
    "        page_content = page_content.split('* Apps')[1]\n",
    "        page_content = page_content.replace('.\\n\\n', '\\n~\\n')\n",
    "        page_content = page_content.replace('\\n\\n', ' ')\n",
    "        page_content = page_content.replace('\\n~\\n', '\\n\\n')\n",
    "        #replace individual newline characters with a space, but leave double newlines or more alone\n",
    "        # doc.page_content = re.sub('\\n{1,99}', ' ', doc.page_content)\n",
    "        \n",
    "        doc = Document(page_content=page_content, metadata=metadata)\n",
    "        docs_transformed.append(doc)\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(\n",
    "    chunked_documents,\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#currently only ('text2text-generation', 'text-generation', 'summarization') are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7346a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently only ('text2text-generation', 'text-generation', 'summarization') are supporte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10398e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CeeDee Lamb plays for the Dallas Cowboys.\n",
      "{'context': [Document(page_content='**Pick: More** ### **CeeDee Lamb (WR -DAL) : 79.5 receiving yards** CeeDee Lamb has caught fire for the Dallas Cowboys. With 117 or more receiving\\nyards in three straight games, CeeDee has upped his season average to 103\\nreceiving yards per game. He has seen insane volume over his last two,\\naveraging 15 targets a game. CeeDee has recorded 77 or more receiving yards in\\nfive of his eight contests this year and is an excellent bet to do so again\\nthis weekend. Lock in the More on CeeDee for Week 10', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='RB — JAC __   4. CeeDee Lamb WR — DAL   5. A.J. Brown WR — PHI   6. Jahmyr Gibbs RB — DET __   7. Tony Pollard RB — DAL   8. Alvin Kamara RB — NO __   9. Austin Ekeler RB — LAC   10. Derrick Henry RB — TEN View all Flex Rankings Fantasy football # Who Should I Start __ __ See Advice  __ ## Popular Searches   * Dak Prescott or Jalen Hurts\\n  * Joshua Dobbs or Jordan Addison\\n  * Alexander Mattison or Khalil Herbert # Product Updates   1. New NFL Touchdown Scorer Reports - Anytime & First TD Prop Bet Analysis Oct 25 — BettingPros 1   2. Major FantasyPros App Update: Win More, Stress Less Oct 09 — FantasyPros.com 2   3. Introducing the New PrizePicks Cheat Sheet Oct 03 — BettingPros 3   4. Game Day 2023: More Features, More Matchup Info, More Fun Sep 07 — FantasyPros.com 4   5. Introducing FantasyPros 2.0: Easier to Use, Easier to Dominate Sep 06 — FantasyPros.com 5 More Updates ## Command Center ## League Hub Navigation Sync your league Sync - Standings - Rankings Playoffs Start/Sit Waiver Trade Player Rankings ## League Hub Console ## League Sync ### Sync your NFL  \\nleague for FREE! Import your fantasy league and get personalized advice for your team', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='RB — JAC __   4. CeeDee Lamb WR — DAL   5. A.J. Brown WR — PHI   6. Jahmyr Gibbs RB — DET __   7. Tony Pollard RB — DAL   8. Alvin Kamara RB — NO __   9. Austin Ekeler RB — LAC   10. Derrick Henry RB — TEN View all Flex Rankings Fantasy football # Who Should I Start __ __ See Advice  __ ## Popular Searches   * Dak Prescott or Jalen Hurts\\n  * Joshua Dobbs or Jordan Addison\\n  * Alexander Mattison or Khalil Herbert # Product Updates   1. New NFL Touchdown Scorer Reports - Anytime & First TD Prop Bet Analysis Oct 25 — BettingPros 1   2. Major FantasyPros App Update: Win More, Stress Less Oct 09 — FantasyPros.com 2   3. Introducing the New PrizePicks Cheat Sheet Oct 03 — BettingPros 3   4. Game Day 2023: More Features, More Matchup Info, More Fun Sep 07 — FantasyPros.com 4   5. Introducing FantasyPros 2.0: Easier to Use, Easier to Dominate Sep 06 — FantasyPros.com 5 More Updates ## Command Center ## League Hub Navigation Sync your league Sync - Standings - Rankings Playoffs Start/Sit Waiver Trade Player Rankings ## League Hub Console ## League Sync ### Sync your NFL  \\nleague for FREE! Import your fantasy league and get personalized advice for your team', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'}), Document(page_content='Remember, he also can be a mobile quarterback, which, if you follow these\\narticles, you know my love of Konomi-code level quarterbacks. The other\\nsignificant factor for Howell is the game narrative. He is a slight underdog,\\naccording to Vegas. This stat is crucial because it means his team must pass\\nto keep up/take the lead. We also know from past research that favorites, not\\nunderdogs, are more heavily rostered in DFS. You can grab yourself a strong\\nquarterback and build leverage with Howell', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'})], 'question': 'What team does CeeDee Lamb play for?', 'text': ' CeeDee Lamb plays for the Dallas Cowboys.'}\n",
      "Total time taken:\t 0:00:01.267143\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain\n",
    "\n",
    "# response = rag_chain.invoke(\"Which players suffered a knee injury?\")\n",
    "# response = rag_chain.invoke(\"What injury did Josh Downs suffer?\")\n",
    "# response = rag_chain.invoke(\"How many yards per game does CeeDee Lamb have?\")\n",
    "# response = rag_chain.invoke(\"What team does CeeDee Lamb play for? And what position does he play? And how many yards per game has he averaged?\")\n",
    "# response = rag_chain.invoke(\"How many touchdowns has Alexander Mattison scored?\")\n",
    "response = rag_chain.invoke(\"What team does CeeDee Lamb play for?\")\n",
    "\n",
    "print(response['text'])\n",
    "print(response)\n",
    "\n",
    "print('Total time taken:\\t', datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CeeDee Lamb plays for the Dallas Cowboys.\n"
     ]
    }
   ],
   "source": [
    "# # Create a deep copy of docs\n",
    "# docs_copy = copy.deepcopy(docs)\n",
    "\n",
    "# # Converts HTML to plain text\n",
    "# html2text = Html2TextTransformer()\n",
    "\n",
    "# docs_transformed = html2text.transform_documents(docs_copy)\n",
    "\n",
    "# for doc in docs_transformed:\n",
    "#     print(doc)\n",
    "\n",
    "print(response['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(page_content='AI/ML Specialist - Full Time/Remote'), Document(page_content='AI & ML expert to identify entry level jobs '), Document(page_content='Machine Learning for ml-cvnets for image prrocessing'), Document(page_content='AI Automation & AI Solutions')], 'question': \"Find jobs similar to: 'AI, machine learning, AI/ML'\", 'text': '\\nBased on your fantasy football knowledge, I would suggest looking for jobs related to AI, machine learning'}\n",
      "\n",
      "Based on your fantasy football knowledge, I would suggest looking for jobs related to AI, machine learning\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.weaviate import Weaviate\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# vectorstore = Weaviate(client, \"Upwork_jobs\", \"snippet\")\n",
    "vectorstore = Weaviate(client, \"Upwork_jobs\", \"job_title\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain\n",
    "\n",
    "\n",
    "# Define the specific job description you're interested in\n",
    "target_job_description = \"AI, machine learning, AI/ML\"\n",
    "\n",
    "# Construct the prompt\n",
    "prompt = f\"Find jobs similar to: '{target_job_description}'\"\n",
    "\n",
    "response = rag_chain.invoke(prompt)\n",
    "print(response)\n",
    "print(response['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = client.schema.get()\n",
    "\n",
    "#write indexes to json\n",
    "with open('indexes.json', 'w') as outfile:\n",
    "    json.dump(indexes, outfile)\n",
    "\n",
    "# !pip show weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models\n",
    "\n",
    "client = weaviate.WeaviateClient(\n",
    "        weaviate.ConnectionParams.from_url(\"http://localhost:8080\", 50051)\n",
    "    )\n",
    "upwork_jobs = client.collections.get(\"Upwork_jobs\")\n",
    "response = upwork_jobs.query.near_text(\n",
    "    query=\"machine learning snippet in the United States\",\n",
    "    limit=25\n",
    ")\n",
    "\n",
    "data = [json.loads(json.dumps(o.properties)) for o in response.objects]\n",
    "# data = {'data': data}\n",
    "# data = '\\n'.join(data)\n",
    "\n",
    "\n",
    "\n",
    "#write response to json file\n",
    "with open('similarity_search.json', 'w') as outfile:\n",
    "    json.dump(data, outfile, indent=2)\n",
    "\n",
    "# for o in response.objects:\n",
    "#     print(json.loads(json.dumps(o.properties)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama2:7b-chat\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
